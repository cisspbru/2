#import includes
import tensorflow as tf
from tensorflow import keras
import numpy as np
import matplotlib.pyplot as plt 
import pandas as pd

from sklearn.model_selection import train_test_split

#read input data
train = pd.read_csv("../input/train.csv")
test = pd.read_csv("../input/test.csv") 

y_train = train["label"]
x_train = train.drop(labels = ["label"], axis = 1)
x_eval = x_train[:4200]

x_eval.describe()

	pixel0 	pixel1 	pixel2 	pixel3 	pixel4 	pixel5 	pixel6 	pixel7 	pixel8 	pixel9 	pixel10 	pixel11 	pixel12 	pixel13 	pixel14 	pixel15 	pixel16 	pixel17 	pixel18 	pixel19 	pixel20 	pixel21 	pixel22 	pixel23 	pixel24 	pixel25 	pixel26 	pixel27 	pixel28 	pixel29 	pixel30 	pixel31 	pixel32 	pixel33 	pixel34 	pixel35 	pixel36 	pixel37 	pixel38 	pixel39 	... 	pixel744 	pixel745 	pixel746 	pixel747 	pixel748 	pixel749 	pixel750 	pixel751 	pixel752 	pixel753 	pixel754 	pixel755 	pixel756 	pixel757 	pixel758 	pixel759 	pixel760 	pixel761 	pixel762 	pixel763 	pixel764 	pixel765 	pixel766 	pixel767 	pixel768 	pixel769 	pixel770 	pixel771 	pixel772 	pixel773 	pixel774 	pixel775 	pixel776 	pixel777 	pixel778 	pixel779 	pixel780 	pixel781 	pixel782 	pixel783
count 	4200.0 	4200.0 	4200.0 	4200.0 	4200.0 	4200.0 	4200.0 	4200.0 	4200.0 	4200.0 	4200.0 	4200.0 	4200.0 	4200.0 	4200.0 	4200.0 	4200.0 	4200.0 	4200.0 	4200.0 	4200.0 	4200.0 	4200.0 	4200.0 	4200.0 	4200.0 	4200.0 	4200.0 	4200.0 	4200.0 	4200.0 	4200.0 	4200.0 	4200.0 	4200.0 	4200.0 	4200.0 	4200.000000 	4200.000000 	4200.000000 	... 	4200.000000 	4200.000000 	4200.000000 	4200.000000 	4200.000000 	4200.000000 	4200.000000 	4200.000000 	4200.0 	4200.0 	4200.0 	4200.0 	4200.0 	4200.0 	4200.0 	4200.0 	4200.0 	4200.000000 	4200.000000 	4200.000000 	4200.000000 	4200.000000 	4200.000000 	4200.000000 	4200.000000 	4200.000000 	4200.000000 	4200.000000 	4200.000000 	4200.000000 	4200.000000 	4200.000000 	4200.000000 	4200.0 	4200.0 	4200.0 	4200.0 	4200.0 	4200.0 	4200.0
mean 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.073333 	0.136429 	0.134762 	... 	3.343810 	2.541190 	1.813571 	1.344762 	0.649524 	0.225952 	0.169286 	0.033333 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.019286 	0.096667 	0.120714 	0.051667 	0.043095 	0.267619 	0.499286 	0.733571 	0.660952 	0.738571 	0.473810 	0.227381 	0.139048 	0.195000 	0.153333 	0.009524 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0
std 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	2.800053 	5.207640 	5.430321 	... 	24.803335 	21.765102 	18.803182 	16.377198 	11.108657 	6.335514 	6.015750 	2.114452 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	1.249857 	4.138434 	4.948359 	2.002724 	1.961356 	6.717006 	9.664237 	12.061967 	10.342172 	11.578891 	9.526728 	5.676100 	4.890289 	6.176327 	5.539424 	0.436384 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0
min 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.000000 	0.000000 	0.000000 	... 	0.000000 	0.000000 	0.000000 	0.000000 	0.000000 	0.000000 	0.000000 	0.000000 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.000000 	0.000000 	0.000000 	0.000000 	0.000000 	0.000000 	0.000000 	0.000000 	0.000000 	0.000000 	0.000000 	0.000000 	0.000000 	0.000000 	0.000000 	0.000000 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0
25% 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.000000 	0.000000 	0.000000 	... 	0.000000 	0.000000 	0.000000 	0.000000 	0.000000 	0.000000 	0.000000 	0.000000 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.000000 	0.000000 	0.000000 	0.000000 	0.000000 	0.000000 	0.000000 	0.000000 	0.000000 	0.000000 	0.000000 	0.000000 	0.000000 	0.000000 	0.000000 	0.000000 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0
50% 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.000000 	0.000000 	0.000000 	... 	0.000000 	0.000000 	0.000000 	0.000000 	0.000000 	0.000000 	0.000000 	0.000000 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.000000 	0.000000 	0.000000 	0.000000 	0.000000 	0.000000 	0.000000 	0.000000 	0.000000 	0.000000 	0.000000 	0.000000 	0.000000 	0.000000 	0.000000 	0.000000 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0
75% 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.000000 	0.000000 	0.000000 	... 	0.000000 	0.000000 	0.000000 	0.000000 	0.000000 	0.000000 	0.000000 	0.000000 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.000000 	0.000000 	0.000000 	0.000000 	0.000000 	0.000000 	0.000000 	0.000000 	0.000000 	0.000000 	0.000000 	0.000000 	0.000000 	0.000000 	0.000000 	0.000000 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0
max 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	132.000000 	253.000000 	255.000000 	... 	255.000000 	254.000000 	255.000000 	254.000000 	254.000000 	254.000000 	253.000000 	137.000000 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	81.000000 	231.000000 	253.000000 	112.000000 	104.000000 	253.000000 	253.000000 	254.000000 	253.000000 	255.000000 	253.000000 	197.000000 	252.000000 	253.000000 	252.000000 	20.000000 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0

x_train.describe()

	pixel0 	pixel1 	pixel2 	pixel3 	pixel4 	pixel5 	pixel6 	pixel7 	pixel8 	pixel9 	pixel10 	pixel11 	pixel12 	pixel13 	pixel14 	pixel15 	pixel16 	pixel17 	pixel18 	pixel19 	pixel20 	pixel21 	pixel22 	pixel23 	pixel24 	pixel25 	pixel26 	pixel27 	pixel28 	pixel29 	pixel30 	pixel31 	pixel32 	pixel33 	pixel34 	pixel35 	pixel36 	pixel37 	pixel38 	pixel39 	... 	pixel744 	pixel745 	pixel746 	pixel747 	pixel748 	pixel749 	pixel750 	pixel751 	pixel752 	pixel753 	pixel754 	pixel755 	pixel756 	pixel757 	pixel758 	pixel759 	pixel760 	pixel761 	pixel762 	pixel763 	pixel764 	pixel765 	pixel766 	pixel767 	pixel768 	pixel769 	pixel770 	pixel771 	pixel772 	pixel773 	pixel774 	pixel775 	pixel776 	pixel777 	pixel778 	pixel779 	pixel780 	pixel781 	pixel782 	pixel783
count 	42000.0 	42000.0 	42000.0 	42000.0 	42000.0 	42000.0 	42000.0 	42000.0 	42000.0 	42000.0 	42000.0 	42000.0 	42000.00000 	42000.000000 	42000.000000 	42000.000000 	42000.0 	42000.0 	42000.0 	42000.0 	42000.0 	42000.0 	42000.0 	42000.0 	42000.0 	42000.0 	42000.0 	42000.0 	42000.0 	42000.0 	42000.0 	42000.0 	42000.000000 	42000.000000 	42000.000000 	42000.000000 	42000.000000 	42000.000000 	42000.000000 	42000.000000 	... 	42000.000000 	42000.000000 	42000.000000 	42000.000000 	42000.000000 	42000.000000 	42000.000000 	42000.000000 	42000.000000 	42000.000000 	42000.0 	42000.0 	42000.0 	42000.0 	42000.0 	42000.0 	42000.0 	42000.000000 	42000.000000 	42000.000000 	42000.000000 	42000.000000 	42000.000000 	42000.000000 	42000.000000 	42000.000000 	42000.000000 	42000.00000 	42000.000000 	42000.000000 	42000.000000 	42000.000000 	42000.000000 	42000.00000 	42000.000000 	42000.000000 	42000.0 	42000.0 	42000.0 	42000.0
mean 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.00300 	0.011190 	0.005143 	0.000214 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.000381 	0.001310 	0.010548 	0.027262 	0.050905 	0.066405 	0.129571 	0.174119 	... 	3.772524 	2.748905 	1.796452 	1.089905 	0.563190 	0.239571 	0.093524 	0.024833 	0.000857 	0.001405 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.006143 	0.035833 	0.082357 	0.114905 	0.178714 	0.301452 	0.413643 	0.513667 	0.558833 	0.677857 	0.60281 	0.489238 	0.340214 	0.219286 	0.117095 	0.059024 	0.02019 	0.017238 	0.002857 	0.0 	0.0 	0.0 	0.0
std 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.56812 	1.626927 	1.053972 	0.043916 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.078072 	0.232634 	1.131661 	2.310396 	3.121847 	3.259128 	4.992894 	5.810949 	... 	26.957829 	22.879248 	18.595109 	14.434439 	10.517823 	6.469315 	3.976306 	1.846016 	0.139556 	0.287891 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.949803 	2.350859 	3.934280 	4.543583 	5.856772 	7.219742 	8.928286 	10.004069 	10.129595 	11.254931 	10.69603 	9.480066 	7.950251 	6.312890 	4.633819 	3.274488 	1.75987 	1.894498 	0.414264 	0.0 	0.0 	0.0 	0.0
min 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.00000 	0.000000 	0.000000 	0.000000 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.000000 	0.000000 	0.000000 	0.000000 	0.000000 	0.000000 	0.000000 	0.000000 	... 	0.000000 	0.000000 	0.000000 	0.000000 	0.000000 	0.000000 	0.000000 	0.000000 	0.000000 	0.000000 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.000000 	0.000000 	0.000000 	0.000000 	0.000000 	0.000000 	0.000000 	0.000000 	0.000000 	0.000000 	0.00000 	0.000000 	0.000000 	0.000000 	0.000000 	0.000000 	0.00000 	0.000000 	0.000000 	0.0 	0.0 	0.0 	0.0
25% 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.00000 	0.000000 	0.000000 	0.000000 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.000000 	0.000000 	0.000000 	0.000000 	0.000000 	0.000000 	0.000000 	0.000000 	... 	0.000000 	0.000000 	0.000000 	0.000000 	0.000000 	0.000000 	0.000000 	0.000000 	0.000000 	0.000000 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.000000 	0.000000 	0.000000 	0.000000 	0.000000 	0.000000 	0.000000 	0.000000 	0.000000 	0.000000 	0.00000 	0.000000 	0.000000 	0.000000 	0.000000 	0.000000 	0.00000 	0.000000 	0.000000 	0.0 	0.0 	0.0 	0.0
50% 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.00000 	0.000000 	0.000000 	0.000000 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.000000 	0.000000 	0.000000 	0.000000 	0.000000 	0.000000 	0.000000 	0.000000 	... 	0.000000 	0.000000 	0.000000 	0.000000 	0.000000 	0.000000 	0.000000 	0.000000 	0.000000 	0.000000 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.000000 	0.000000 	0.000000 	0.000000 	0.000000 	0.000000 	0.000000 	0.000000 	0.000000 	0.000000 	0.00000 	0.000000 	0.000000 	0.000000 	0.000000 	0.000000 	0.00000 	0.000000 	0.000000 	0.0 	0.0 	0.0 	0.0
75% 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.00000 	0.000000 	0.000000 	0.000000 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.000000 	0.000000 	0.000000 	0.000000 	0.000000 	0.000000 	0.000000 	0.000000 	... 	0.000000 	0.000000 	0.000000 	0.000000 	0.000000 	0.000000 	0.000000 	0.000000 	0.000000 	0.000000 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.000000 	0.000000 	0.000000 	0.000000 	0.000000 	0.000000 	0.000000 	0.000000 	0.000000 	0.000000 	0.00000 	0.000000 	0.000000 	0.000000 	0.000000 	0.000000 	0.00000 	0.000000 	0.000000 	0.0 	0.0 	0.0 	0.0
max 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	116.00000 	254.000000 	216.000000 	9.000000 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	16.000000 	47.000000 	157.000000 	254.000000 	255.000000 	243.000000 	255.000000 	255.000000 	... 	255.000000 	255.000000 	255.000000 	255.000000 	255.000000 	255.000000 	255.000000 	253.000000 	28.000000 	59.000000 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	0.0 	177.000000 	231.000000 	253.000000 	254.000000 	254.000000 	255.000000 	255.000000 	255.000000 	255.000000 	255.000000 	255.00000 	255.000000 	255.000000 	254.000000 	254.000000 	253.000000 	253.00000 	254.000000 	62.000000 	0.0 	0.0 	0.0 	0.0

y_train.describe()

count    42000.000000
mean         4.456643
std          2.887730
min          0.000000
25%          2.000000
50%          4.000000
75%          7.000000
max          9.000000
Name: label, dtype: float64

#data normalization  in range 0..1
x_train /=255
test /=255

x_train = x_train.values.reshape(-1,28,28,1)
test = test.values.reshape(-1,28,28,1)

x_eval = x_train[:4200]
x_train = x_train[4200:]
y_eval = y_train[:4200]
y_train = y_train[4200:]

aug_data = keras.preprocessing.image.ImageDataGenerator(
        featurewise_center=False,  # set input mean to 0 over the dataset
        samplewise_center=False,  # set each sample mean to 0
        featurewise_std_normalization=False,  # divide inputs by std of the dataset
        samplewise_std_normalization=False,  # divide each input by its std
        zca_whitening=False,  # apply ZCA whitening
        rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)
        zoom_range = 0.1, # Randomly zoom image 
        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)
        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)
        horizontal_flip=False,  # randomly flip images
        vertical_flip=False)  # randomly flip images)
batches = aug_data.flow(x_train, y_train, batch_size = 86)

model = keras.Sequential([
    keras.layers.Conv2D(256, (3,3),padding = 'Same', activation=tf.nn.relu, input_shape=(28,28,1)),
    keras.layers.MaxPool2D(pool_size=(2,2)),
    keras.layers.Dropout(0.25),
    keras.layers.Conv2D(128, (3,3),padding = 'Same', activation=tf.nn.relu),
    keras.layers.MaxPool2D(pool_size=(2,2)),
    keras.layers.Flatten(),
    keras.layers.Dropout(0.25),
    keras.layers.Dense(256, activation=tf.nn.relu),
    keras.layers.Dense(10, activation=tf.nn.softmax)    
]) 

WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.
Instructions for updating:
Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.

learning_rate_reduction = keras.callbacks.ReduceLROnPlateau(monitor='val_acc', 
                                            patience=3, 
                                            verbose=1, 
                                            factor=0.5)

model.compile(optimizer=tf.train.AdamOptimizer(),
             loss='sparse_categorical_crossentropy',
             metrics=['accuracy'])

#model.compile(optimizer=keras.optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0),loss='sparse_categorical_crossentropy', metrics=['accuracy'])

#model.fit(x_train, y_train, epochs=200, validation_split = 0.1)
#callbacks=[learning_rate_reduction],
history = model.fit_generator(generator=batches,steps_per_epoch=batches.n,  epochs=200)

Epoch 1/200
440/440 [==============================] - 15s 35ms/step - loss: 0.3008 - acc: 0.9035
Epoch 2/200
440/440 [==============================] - 15s 33ms/step - loss: 0.0994 - acc: 0.9688
Epoch 3/200
440/440 [==============================] - 15s 33ms/step - loss: 0.0736 - acc: 0.9771
Epoch 4/200
440/440 [==============================] - 14s 33ms/step - loss: 0.0616 - acc: 0.9816
Epoch 5/200
440/440 [==============================] - 14s 33ms/step - loss: 0.0547 - acc: 0.9833
Epoch 6/200
440/440 [==============================] - 14s 33ms/step - loss: 0.0503 - acc: 0.9844
Epoch 7/200
440/440 [==============================] - 14s 32ms/step - loss: 0.0440 - acc: 0.9860
Epoch 8/200
440/440 [==============================] - 14s 33ms/step - loss: 0.0433 - acc: 0.9863
Epoch 9/200
440/440 [==============================] - 14s 33ms/step - loss: 0.0415 - acc: 0.9866
Epoch 10/200
440/440 [==============================] - 14s 33ms/step - loss: 0.0361 - acc: 0.9884
Epoch 11/200
440/440 [==============================] - 14s 33ms/step - loss: 0.0366 - acc: 0.9887
Epoch 12/200
440/440 [==============================] - 14s 33ms/step - loss: 0.0337 - acc: 0.9894
Epoch 13/200
440/440 [==============================] - 15s 33ms/step - loss: 0.0318 - acc: 0.9890
Epoch 14/200
440/440 [==============================] - 15s 33ms/step - loss: 0.0340 - acc: 0.9892
Epoch 15/200
440/440 [==============================] - 14s 33ms/step - loss: 0.0282 - acc: 0.9910
Epoch 16/200
440/440 [==============================] - 14s 33ms/step - loss: 0.0307 - acc: 0.9897
Epoch 17/200
440/440 [==============================] - 15s 33ms/step - loss: 0.0265 - acc: 0.9921
Epoch 18/200
440/440 [==============================] - 14s 33ms/step - loss: 0.0283 - acc: 0.9913
Epoch 19/200
440/440 [==============================] - 14s 33ms/step - loss: 0.0262 - acc: 0.9911
Epoch 20/200
440/440 [==============================] - 14s 32ms/step - loss: 0.0263 - acc: 0.9916
Epoch 21/200
440/440 [==============================] - 14s 33ms/step - loss: 0.0257 - acc: 0.9917
Epoch 22/200
440/440 [==============================] - 14s 33ms/step - loss: 0.0244 - acc: 0.9923
Epoch 23/200
440/440 [==============================] - 14s 32ms/step - loss: 0.0218 - acc: 0.9931
Epoch 24/200
440/440 [==============================] - 14s 32ms/step - loss: 0.0215 - acc: 0.9930
Epoch 25/200
440/440 [==============================] - 14s 32ms/step - loss: 0.0223 - acc: 0.9930
Epoch 26/200
440/440 [==============================] - 14s 32ms/step - loss: 0.0243 - acc: 0.9919
Epoch 27/200
223/440 [==============>...............] - ETA: 7s - loss: 0.0208 - acc: 0.9935

plt.plot(history.history['loss'], color='r', label="Train Loss")
plt.title("Train Loss")
plt.xlabel("Number of Epochs")
plt.ylabel("Loss")
plt.legend()
plt.show()

plt.plot(history.history['acc'], color='g', label='Train Accuracy')
plt.title('Train Accuracy')
plt.xlabel('Number of Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

test_loss, test_acc = model.evaluate(x_eval, y_eval)
print('accuracy:', test_acc)

4200/4200 [==============================] - 0s 109us/sample - loss: 0.0214 - acc: 0.9940
accuracy: 0.99404764

output = model.predict_classes(test, verbose=1)

28000/28000 [==============================] - 2s 64us/sample

file = pd.DataFrame({"ImageId":list(range(1,len(output)+1)),"Label":output})

file.to_csv('submission.csv',index=False,header=True)

